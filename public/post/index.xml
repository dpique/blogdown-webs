<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Daniel Pique&#39;s website</title>
    <link>/post/</link>
    <description>Recent content in Posts on Daniel Pique&#39;s website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 May 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Instrumental and latent variables</title>
      <link>/2018/05/29/instrumental-and-latent-variables/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/29/instrumental-and-latent-variables/</guid>
      <description>Resources for understanding instrumental variables:
http://www.rebeccabarter.com/blog/2018-05-23-instrumental_variables/
Great youtube series (3 videos): https://www.youtube.com/watch?v=bSLEGjbuit8 https://www.youtube.com/watch?v=6tyO4ISzVhw https://www.youtube.com/watch?v=OwM3BgWEgUg
Confounding and causal inference: http://www.rebeccabarter.com/blog/2017-07-05-confounding/</description>
    </item>
    
    <item>
      <title>pdf, cdf, and inverse cdf</title>
      <link>/2018/05/19/pdf-cdf-and-inverse-cdf/</link>
      <pubDate>Sat, 19 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/19/pdf-cdf-and-inverse-cdf/</guid>
      <description>In this post, we will cover the relationship between the probability density function (PDF), the cumulative probability density function (CDF), and the inverse CDF. The inverse CDF is also know as the Quantile function (we’ll see why later).
This post stemmed from my interest in transforming a set of values on a [0,1] scale to values corresponsing to those on a Normal distribution. This led to me learning about something called the inverse transform method.</description>
    </item>
    
    <item>
      <title>Deriving Least Squares Solution for Linear Regression</title>
      <link>/2018/04/21/deriving-least-squares-solution-for-linear-regression/</link>
      <pubDate>Sat, 21 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/21/deriving-least-squares-solution-for-linear-regression/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Solving Systems of Equations in R</title>
      <link>/2018/04/21/solving-systems-of-equations-in-r/</link>
      <pubDate>Sat, 21 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/21/solving-systems-of-equations-in-r/</guid>
      <description>library(tidyverse) ## ── Attaching packages ─────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.2.0 ## ── Conflicts ────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() The code shown here is derived from this video: https://www.youtube.com/watch?v=jF71LzipydA
Imagine that we have a system of equations with 3 variables and 3 unknowns.</description>
    </item>
    
    <item>
      <title>Computational resources and best practices for your PhD</title>
      <link>/2018/04/07/computational-resources-and-best-practices-for-your-phd/</link>
      <pubDate>Sat, 07 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/07/computational-resources-and-best-practices-for-your-phd/</guid>
      <description>Below are some best practices and computational resources offered by Einstein, along with links to how you can obtain these resources. I divide these resources/best practices into essentials, for every PhD student, and useful/non-essential, which will be useful mainly for those doing computationally intensive work. Einstein’s resources are a bit spread out across multiple websites, but hopefully this list helps synthesize these different tips and resources. I also include some freely available resources not specific to Einstein but that may be helpful to students generally.</description>
    </item>
    
    <item>
      <title>How to access and work on Einstein&#39;s HPC cluster</title>
      <link>/2017/11/29/introduction-to-einstein-s-hpc-cluster/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/29/introduction-to-einstein-s-hpc-cluster/</guid>
      <description>Request access to the cluster. This can be done by emailing Brent Solly or Shailesh Shenoy and asking to be added as a user. An account and userid will be created for you. You can also submit a ticket through the IT support portal to request account access.
 Open up a terminal (Mac/Linux) or command prompt (Windows)
 In the prompt, type ssh user_id@ip_address, where user_id is your montefiore ID/username, and ip_address is the IP address shared with you by the IT team (should start with the number 10 and be in the following format: 10.</description>
    </item>
    
    <item>
      <title>logit</title>
      <link>/2017/11/28/logit/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/28/logit/</guid>
      <description>logit(p) &amp;lt;- log(p/(1-p)) #this is appropriately called the log odds. The “odds” are a ratio between 2 events – the probability of the event happening (p) and not happening (1-p). We see this as p/(1-p). The odds are wrapped in a logarithm – hence the log-odds.
It is used to transform probabilities on the open interval (0,1) to the Real number line.
The distribution of many of these mapped probability values is often assumed to have a Normal distribution.</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>/2017/11/24/principal-component-analysis/</link>
      <pubDate>Fri, 24 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/24/principal-component-analysis/</guid>
      <description>The purpose of this tutorial is to understand PCA and to be able to carry out the basic visualizations associated with PCA in R. What is the purpose of PCA? In two words: dimensionality reduction. Basically, we use linear algebra to transform the original variables into a new set of variables called Principal Components (PCs). The number of original observations and variables stays the same, but the new variables are ordered by how much variance they explain in the data.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description>How to access and work on Einstein’s HPC cluster      h1 { font-size: 34px; } h1.title { font-size: 38px; } h2 { font-size: 30px; } h3 { font-size: 24px; } h4 { font-size: 18px; } h5 { font-size: 16px; } h6 { font-size: 12px; } .table th:not([align]) { text-align: left; }    .main-container { max-width: 940px; margin-left: auto; margin-right: auto; } code { color: inherit; background-color: rgba(0, 0, 0, 0.</description>
    </item>
    
  </channel>
</rss>