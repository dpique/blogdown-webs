<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.20.6" />


<title>Principal Component Analysis - Daniel Pique&#39;s website</title>
<meta property="og:title" content="Principal Component Analysis - Daniel Pique&#39;s website">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/cv/">CV</a></li>
    
    <li><a href="https://github.com/dpique/">GitHub</a></li>
    
    <li><a href="/resources/">Resources</a></li>
    
    <li><a href="https://twitter.com/dpique12">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">9 min read</span>
    

    <h1 class="article-title">Principal Component Analysis</h1>

    
    <span class="article-date">2017/11/24</span>
    

    <div class="article-content">
      <div id="the-purpose-of-this-tutorial-is-to-understand-pca-and-to-be-able-to-carry-out-the-basic-visualizations-associated-with-pca-in-r." class="section level2">
<h2>The purpose of this tutorial is to understand PCA and to be able to carry out the basic visualizations associated with PCA in R.</h2>
<div id="what-is-the-purpose-of-pca" class="section level3">
<h3>What is the purpose of PCA?</h3>
<p>In two words: dimensionality reduction. Basically, we use linear algebra to transform the original variables into a new set of variables called Principal Components (PCs). The number of original observations and variables stays the same, but the new variables are ordered by how much variance they explain in the data. This data matrix D is an N x P matrix, with each row corresponding to one of N samples/observations, and each of P variables as columns.</p>
<ul>
<li>The covariance between two normalized variables <em>is</em> the correlation coefficient.</li>
</ul>
<p>How do we conduct PCA in R?</p>
<ol style="list-style-type: decimal">
<li>First, we need some data, <code>d</code>.</li>
</ol>
<pre class="r"><code>d = matrix(rnorm(40, 4, 5), nrow = 10, ncol = 4)
rownames(d) = paste0(&quot;Pt_&quot;, letters[1:10])
colnames(d) = paste0(&quot;Var_&quot;, 1:4)
d #a matrix with 10 observations and 4 variables</code></pre>
<pre><code>##           Var_1     Var_2      Var_3     Var_4
## Pt_a  6.2823342 -1.289626  1.4576785  4.466958
## Pt_b  8.2865135 -6.990068  9.5393968  2.244894
## Pt_c  4.1759034 -1.214622 -1.2072433  8.676498
## Pt_d  3.6915884 -1.348987  4.5268741 12.164401
## Pt_e  8.8867650  5.388644  6.4755139  1.681967
## Pt_f  9.1691437  1.454690  3.6501919 10.820894
## Pt_g  3.1365921  1.141671 -0.3129707  8.512976
## Pt_h -0.4548564  7.860579  7.2072110  1.382484
## Pt_i  3.9510410 11.119503  9.3520675  4.116189
## Pt_j  3.0438303 13.264645  6.9312697 10.223448</code></pre>
<p>In this case, we have 10 patients (observations), <code>Pt_1</code> through <code>Pt_10</code>, and 4 variables, <code>Var_1</code> through <code>Var_4</code>. These variables are gene expression levels from 4 genes measured from each patient’s blood.</p>
<ol start="2" style="list-style-type: decimal">
<li>Our next step is to standardize our variables. This includes, for each variable, centering the mean around 0 and scaling the variance to equal 1. Depending on your data, a <a href="https://www.isixsigma.com/tools-templates/normality/tips-recognizing-and-transforming-non-normal-data/">log, Box-Cox, or other transformation</a> may be required to make the variables look more like a standard normal distribution. Why? <a href="https://www.quora.com/Are-there-implicit-Gaussian-assumptions-in-the-use-of-PCA-principal-components-analysis">PCA works best</a> when the variables have a joint multivariate Normal (Gaussian) distribution, though this does not stop you from applying PCA to non-normal data, such as genotype data (which consists of the integers 0, 1, &amp; 2).</li>
</ol>
<pre class="r"><code>dctr = apply(d, 2, function(x) x - mean(x)) #mean-center the variables
dctr.std = apply(dctr, 2, function(x) x / sd(x))</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Next, we calculate the covariance of <code>dctr.std</code>, and observe that it is equivalent to the correlation matrix when the variables have a mean of 0 and standard deviation 1. Note that the variance of each variable is shown along the main diagonal, and the covariance of each variable with the other variables is shown in the off diagonal.</li>
</ol>
<pre class="r"><code>dctr.std.c = cov(dctr.std)
dctr.std.c #same as cor matrix with unit std dev &amp; 0 mean</code></pre>
<pre><code>##             Var_1       Var_2       Var_3       Var_4
## Var_1  1.00000000 -0.44364968  0.05455704 -0.06269753
## Var_2 -0.44364968  1.00000000  0.36431104 -0.04524648
## Var_3  0.05455704  0.36431104  1.00000000 -0.47374677
## Var_4 -0.06269753 -0.04524648 -0.47374677  1.00000000</code></pre>
<pre class="r"><code>all.equal(dctr.std.c, cor(dctr.std))</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>There are two important things to note about the <code>dctr.std.c</code> covariance matrix: - 1. It is square (<span class="math inline">\(P x P\)</span>). This means that we can apply techniques that describe square matrices, such as eigenvalues and eigenvectors (Step 4). Note that the eigenvalues are closely related to the trace, determinant, and rank, all of which are additional features of square matrices. - 2. It is symmetric (<code>dctr.std.c = t(dctr.std.c)</code>) about the diagonal. It is also positive semi-definite, which is a common type of matrix in statistics to compute on.</p>
<ol start="4" style="list-style-type: decimal">
<li>The next step is to extract the eigenvalues &amp; eigenvectors of the covariance matrix <code>dctr.std.c</code>. We do that in 1 step with <code>eigen</code>.</li>
</ol>
<pre class="r"><code>d.c.e = eigen(dctr.std.c)

pca = prcomp(dctr.std)</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>We’ll then inspect the eigenvectors. There should be as many eigenvectors as there are original variables, and the eigenvectors are all perpendicular to each other. The length of the eigenvector is 1, by convention. This is similar to pythagoras’ theorem, whereby the sum of the squares gives you the Euclidian distance (L2 norm)</li>
</ol>
<pre class="r"><code>## first we look at the eigenvectors of the covariance matrix.
## each column in this matrix is an eigenvector
d.c.e$vectors</code></pre>
<pre><code>##            [,1]       [,2]      [,3]       [,4]
## [1,]  0.2881146  0.6711658 0.5494687 -0.4057224
## [2,] -0.5685421 -0.4291336 0.3928252 -0.5816292
## [3,] -0.6199300  0.3331537 0.3671862  0.6081691
## [4,]  0.4576424 -0.5043703 0.6394895  0.3566892</code></pre>
<pre class="r"><code>sqrt(sum(d.c.e$vectors[,1]^2)) #length of 1st eigenvector = 1</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>sqrt(sum(d.c.e$vectors[,2]^2)) #length of 2nd eigenvector = 1</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>## the dot product of any 2 eigenvectors is 0
## which means they are perpendicular to each other
d.c.e$vectors[,1] %*% d.c.e$vectors[,2]</code></pre>
<pre><code>##             [,1]
## [1,] 1.94289e-16</code></pre>
<ul>
<li>Another name for the covariance matrix <code>dctr.std.c</code> is the <strong>transformation matrix</strong>, which transforms the coordinate space. We are looking for column vectors (eigenvectors) of <code>d.c.e</code> that, when multiplied on the right of the covariance (transformation) matrix, yield the same vector, but scaled. The degree to which they are scaled are the eigenvalues. This is the same as saying <span class="math inline">\(A\lambda = c\lambda\)</span>, where c is a constant equal to the eigenvalue. A is the transformation matrix, and lambda is the eigenvector. This is illustrated below with the first eigenvector/value:</li>
</ul>
<pre class="r"><code>## the covariance of the data * the 1st eigenvector...
dctr.std.c %*% matrix(d.c.e$vectors[,1], ncol=1)</code></pre>
<pre><code>##             [,1]
## Var_1  0.4778336
## Var_2 -0.9429182
## Var_3 -1.0281441
## Var_4  0.7589927</code></pre>
<pre class="r"><code>##...equals the scale factor * the 1st eigenvector
d.c.e$values[1] * matrix(d.c.e$vectors[,1], ncol=1) </code></pre>
<pre><code>##            [,1]
## [1,]  0.4778336
## [2,] -0.9429182
## [3,] -1.0281441
## [4,]  0.7589927</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>We’ll now inspect the eigenvalues of the covariance matrix. The eigenvalues are linearly decreasing, positive numbers. <a href="https://www.youtube.com/watch?v=5zk93CpKYhg">The normalized sum of the eigenvalues</a> represents the variance explained by each eigenvector.</li>
</ol>
<pre class="r"><code>#eigenvalues
d.c.e$values </code></pre>
<pre><code>## [1] 1.6584842 1.3578604 0.6463154 0.3373400</code></pre>
<pre class="r"><code>#plot the percent variance explained by each PC
perc_var &lt;- d.c.e$values/sum(d.c.e$values)
plot(1:length(perc_var), perc_var, xlab=&quot;index&quot;, ylab=&quot;% variance&quot;)</code></pre>
<p><img src="/post/2017-11-24-principal-component-analysis_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<ol start="7" style="list-style-type: decimal">
<li>Now that we have these eigenvectors (ie new orthogonal coordinates that describe the variance in the data), we must transform our original data into this new coordinate space. Our new data will have the same dimensions. To do this, we’ll multiply our the original scaled dataset by the eigenvectors. If we wanted only 2 variables (for visualization purposes) we could also do that.</li>
</ol>
<pre class="r"><code>## centered variables * eigenvectors
## should equal prcomp(dctr.std)$x
new.vars = dctr.std %*% d.c.e$vectors

## This works too:
## new.vars.3 = dctr.std %*% d.c.e$vectors[1:3,]</code></pre>
<ol start="8" style="list-style-type: decimal">
<li>There are a few interesting properties of these new variables. First, these new variables are all uncorrelated with each other, as can be seen with the 0’s in the off diagonals in the covariance matrix. The variance of each new variable can be found in the diagonal entries in the covariance matrix between all values. The variance of each variable is also equal to the eigenvalues. Each variable is still centered at 0 as well.</li>
</ol>
<pre class="r"><code>#all the new variables are uncorrelated (0 covariance!)
round(cov(new.vars), 3) </code></pre>
<pre><code>##       [,1]  [,2]  [,3]  [,4]
## [1,] 1.658 0.000 0.000 0.000
## [2,] 0.000 1.358 0.000 0.000
## [3,] 0.000 0.000 0.646 0.000
## [4,] 0.000 0.000 0.000 0.337</code></pre>
<pre class="r"><code>var(new.vars[,1])</code></pre>
<pre><code>## [1] 1.658484</code></pre>
<pre class="r"><code>d.c.e$values</code></pre>
<pre><code>## [1] 1.6584842 1.3578604 0.6463154 0.3373400</code></pre>
<pre class="r"><code>cov(prcomp(dctr.std)$x) #this is the same as the line above</code></pre>
<pre><code>##               PC1           PC2           PC3           PC4
## PC1  1.658484e+00  2.481377e-16  3.539559e-16 -1.153910e-16
## PC2  2.481377e-16  1.357860e+00  2.172862e-16 -3.036777e-16
## PC3  3.539559e-16  2.172862e-16  6.463154e-01 -3.746483e-17
## PC4 -1.153910e-16 -3.036777e-16 -3.746483e-17  3.373400e-01</code></pre>
<pre class="r"><code>diag(prcomp(dctr.std)$sdev^2) #this is the same as the line above</code></pre>
<pre><code>##          [,1]    [,2]      [,3]    [,4]
## [1,] 1.658484 0.00000 0.0000000 0.00000
## [2,] 0.000000 1.35786 0.0000000 0.00000
## [3,] 0.000000 0.00000 0.6463154 0.00000
## [4,] 0.000000 0.00000 0.0000000 0.33734</code></pre>
<pre class="r"><code>colSums(new.vars) #equal 0, so these are still 0-centered!</code></pre>
<pre><code>## [1]  6.661338e-16  2.775558e-16  3.885781e-16 -1.054712e-15</code></pre>
<ol start="9" style="list-style-type: decimal">
<li>So we performed a linear transformation of our original scaled variables into this new coordinate space. Shouldn’t we be able to transform these variables back into the original scaled variables? Yes!</li>
</ol>
<pre class="r"><code>## how do we get back the original data from eigenvectors/values?
## multiply the eigenvectors by the rotated variables
all.equal(t(d.c.e$vectors %*% t(new.vars)), dctr.std)</code></pre>
<pre><code>## [1] &quot;Attributes: &lt; Component \&quot;dimnames\&quot;: Component 2: target is NULL, current is character &gt;&quot;</code></pre>
<pre class="r"><code>## returns the original data
all.equal(t(d.c.e$vectors %*% t(new.vars))+ d - dctr.std, d)</code></pre>
<pre><code>## [1] &quot;Attributes: &lt; Component \&quot;dimnames\&quot;: Component 2: target is NULL, current is character &gt;&quot;</code></pre>
<ol start="10" style="list-style-type: decimal">
<li>Here are a few things to remember about linear transformations:</li>
</ol>
<ul>
<li>Linear transformations are the specialty of <em>linear</em> algebra (as opposed to <em>nonlinear</em> transformations)</li>
<li>Linear transformations are represented by matrices, known as transformation matrix (TMs).</li>
<li>TMs, when multiplied to the left of vectors that have unit variance, transform the vector into the new transformed coordinate space (e.g. shear in 2d)</li>
<li>The covariance matrix functions as a transformation matrix.</li>
</ul>
<ol start="11" style="list-style-type: decimal">
<li>The whole pca procedure can be implemented with built in functions in R or by using the singular value decomposition on the covariance matrix.</li>
</ol>
<pre class="r"><code>pca = prcomp(d, center = T, scale. = T)
all.equal(pca$rotation %*% t(pca$x) , t(dctr.std))</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>#these are almost equal, the 1st column of pca$x is *-1 
#of the 1st column of new.vars
pca$x; new.vars</code></pre>
<pre><code>##              PC1        PC2        PC3         PC4
## Pt_a -0.81496064  0.5154098  0.6610642  0.47600833
## Pt_b  0.04733438  2.3173043  0.2269441 -0.87747132
## Pt_c -1.51495949 -0.6999172  0.6304575  0.26325395
## Pt_d -0.94049376 -0.7257171 -0.3716153 -1.03056978
## Pt_e  0.66730367  1.4115783 -0.2648126  0.87516311
## Pt_f -1.19313454  0.3664000 -1.2277490  0.20473686
## Pt_g -1.04282088 -0.9876388  0.6091982  0.21448997
## Pt_h  1.91479641 -0.6895177  1.2239081 -0.21823821
## Pt_i  1.83794437 -0.1004614 -0.3972763  0.08089827
## Pt_j  1.03899048 -1.4074401 -1.0901189  0.01172882</code></pre>
<pre><code>##             [,1]       [,2]       [,3]        [,4]
## Pt_a  0.81496064  0.5154098 -0.6610642 -0.47600833
## Pt_b -0.04733438  2.3173043 -0.2269441  0.87747132
## Pt_c  1.51495949 -0.6999172 -0.6304575 -0.26325395
## Pt_d  0.94049376 -0.7257171  0.3716153  1.03056978
## Pt_e -0.66730367  1.4115783  0.2648126 -0.87516311
## Pt_f  1.19313454  0.3664000  1.2277490 -0.20473686
## Pt_g  1.04282088 -0.9876388 -0.6091982 -0.21448997
## Pt_h -1.91479641 -0.6895177 -1.2239081  0.21823821
## Pt_i -1.83794437 -0.1004614  0.3972763 -0.08089827
## Pt_j -1.03899048 -1.4074401  1.0901189 -0.01172882</code></pre>
<pre class="r"><code>#svd on covariance matrix
dctr.c.svd = svd(dctr.std.c) 
all.equal(dctr.c.svd$d, d.c.e$values) #eigenvalues, same as d.c.e$values</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>#these are almost equal, one is the negative of the other
dctr.c.svd$u; d.c.e$vectors</code></pre>
<pre><code>##            [,1]       [,2]       [,3]       [,4]
## [1,] -0.2881146  0.6711658 -0.5494687 -0.4057224
## [2,]  0.5685421 -0.4291336 -0.3928252 -0.5816292
## [3,]  0.6199300  0.3331537 -0.3671862  0.6081691
## [4,] -0.4576424 -0.5043703 -0.6394895  0.3566892</code></pre>
<pre><code>##            [,1]       [,2]      [,3]       [,4]
## [1,]  0.2881146  0.6711658 0.5494687 -0.4057224
## [2,] -0.5685421 -0.4291336 0.3928252 -0.5816292
## [3,] -0.6199300  0.3331537 0.3671862  0.6081691
## [4,]  0.4576424 -0.5043703 0.6394895  0.3566892</code></pre>
<ul>
<li>Scatterplot of new variables</li>
</ul>
<pre class="r"><code>plot(new.vars[1,], new.vars[2,], xlab=paste0(&quot;PC1: &quot;, round(perc_var[1],2)*100, &quot;% of variance&quot;), ylab=paste0(&quot;PC2: &quot;, round(perc_var[2],2)*100, &quot;% of variance&quot;))</code></pre>
<p><img src="/post/2017-11-24-principal-component-analysis_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<ul>
<li>BiPlots - projecting the original variable directions onto the new PC space.</li>
</ul>
<p><a href="https://youtu.be/JEYLfIVvR9I" class="uri">https://youtu.be/JEYLfIVvR9I</a></p>
<pre class="r"><code>x &lt;- pca
scores &lt;- x$x
choices=1L:2L
scale = 1
n &lt;- NROW(scores)

lam &lt;- x$sdev[choices]
lam &lt;- lam * sqrt(n)
lam &lt;- lam^scale
scores[,choices]/lam</code></pre>
<pre><code>##              PC1         PC2
## Pt_a -0.20011561  0.12656015
## Pt_b  0.01284544  0.62886223
## Pt_c -0.37200207 -0.17186641
## Pt_d -0.25522803 -0.19694266
## Pt_e  0.16385807  0.34661656
## Pt_f -0.32378883  0.09943239
## Pt_g -0.25606726 -0.24251716
## Pt_h  0.51963099 -0.18711899
## Pt_i  0.45131181 -0.02466855
## Pt_j  0.28195773 -0.38194636</code></pre>
<pre class="r"><code>t(t(scores[, choices])/lam) </code></pre>
<pre><code>##              PC1         PC2
## Pt_a -0.20011561  0.13987017
## Pt_b  0.01162307  0.62886223
## Pt_c -0.37200207 -0.18994118
## Pt_d -0.23094058 -0.19694266
## Pt_e  0.16385807  0.38306936
## Pt_f -0.29297715  0.09943239
## Pt_g -0.25606726 -0.26802208
## Pt_h  0.47018301 -0.18711899
## Pt_i  0.45131181 -0.02726288
## Pt_j  0.25512669 -0.38194636</code></pre>
<pre class="r"><code># samples in PC space
#divided by lambda
t(t(x$rotation[, choices]) * lam) </code></pre>
<pre><code>##             PC1       PC2
## Var_1 -1.173332  2.473189
## Var_2  2.315359 -1.581321
## Var_3  2.524633  1.227643
## Var_4 -1.863725 -1.858562</code></pre>
<pre class="r"><code>#original variable directions in PC space
#multiplied by lambda</code></pre>
<ul>
<li>Principle component permutation</li>
</ul>
</div>
</div>
<div id="additional-resources-on-pca" class="section level2">
<h2>Additional resources on PCA</h2>
<div id="videos" class="section level3">
<h3>Videos</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=ZqXnPcyIAL8" class="uri">https://www.youtube.com/watch?v=ZqXnPcyIAL8</a></li>
<li><a href="https://youtu.be/_UVHneBUBW0" class="uri">https://youtu.be/_UVHneBUBW0</a></li>
<li><a href="https://www.youtube.com/watch?v=5zk93CpKYhg" class="uri">https://www.youtube.com/watch?v=5zk93CpKYhg</a></li>
</ul>
</div>
<div id="blog-tutorials" class="section level3">
<h3>Blog tutorials</h3>
<p><a href="http://www.lauradhamilton.com/introduction-to-principal-component-analysis-pca" class="uri">http://www.lauradhamilton.com/introduction-to-principal-component-analysis-pca</a> <a href="https://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-principal-component-analysis-pca" class="uri">https://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-principal-component-analysis-pca</a></p>
</div>
<div id="papers" class="section level3">
<h3>Papers</h3>
<p><a href="https://www.cs.cmu.edu/~elaw/papers/pca.pdf" class="uri">https://www.cs.cmu.edu/~elaw/papers/pca.pdf</a> <a href="http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/pca_tutorial.pdf" class="uri">http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/pca_tutorial.pdf</a></p>
</div>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//blogdown-webs.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'blogdown-webs';
    var disqus_identifier = '\/2017\/11\/24\/principal-component-analysis\/';
    var disqus_title = 'Principal Component Analysis';
    var disqus_url = '\/2017\/11\/24\/principal-component-analysis\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </body>
</html>

